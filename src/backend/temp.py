# -*- coding: utf-8 -*-
"""identifica volti.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EJRICW3PybVcdoy2ZueI5se7RFvyCHoA
"""


"""# installazione librerie

https://prod.liveshare.vsengsaas.visualstudio.com/join?43CB9FBE5554C449E7911F923A932FD82B21

https://studio.firebase.google.com/test-60251688
"""

"""# Librerie  """

import os
import cv2
from ultralytics import YOLO
from deepface import DeepFace
import shutil
import yt_dlp
import wget
import ffmpeg
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle
from huggingface_hub import snapshot_download
from sklearn.metrics.pairwise import cosine_similarity
import shutil
import glob
import time

"""# Costanti"""

DIRVOLTI = 'volti'
PATHVIDEO = 'video.mp4'
PATHVIDEOTAGLIATO = 'videoTagliato.mp4'
URLVIDEO = "https://www.youtube.com/watch?v=6U4-KZSoe6g"
URLIMMAGINI = {
    'Jim Carrey':'https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fm.media-amazon.com%2Fimages%2FM%2FMV5BMTQwMjAwNzI0M15BMl5BanBnXkFtZTcwOTY1MTMyOQ%40%40._V1_.jpg&f=1&nofb=1&ipt=b9e9bcc32daa1131841e8f37fd79e22018c58dcb9948134a52a2595bea921bf0',
    'Laura Linney':'https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fm.media-amazon.com%2Fimages%2FM%2FMV5BMTMyMDc3Mzc2M15BMl5BanBnXkFtZTcwMjc5OTcyMg%40%40._V1_.jpg&f=1&h=400',
    'lukaku':'https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.thefamouspeople.com%2Fprofiles%2Fimages%2Fromelu-lukaku-5.jpg&f=1&nofb=1&ipt=cd979c161ffcc8d75cb1be8bb7da25b84693dcfe67f9371e92c8f43be1f83867',
    'conte':'https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.baritalianews.it%2Fwp-content%2Fuploads%2F2014%2F05%2Fconte.jpg&f=1&nofb=1&ipt=81a1f0aa719ae9b154d7ee8d45c1ccd2ae975535967f84dd7e8762a636843881',
}
TEMPIMG = 'temp_images/temp.jpg'
FRAMEDASALTARE = 3
SECONDOINIZIO = 14
DURATAVIDEO = 14

"""# Creazioni cartelle"""

os.makedirs(DIRVOLTI, exist_ok=True)
os.makedirs(TEMPIMG.split('/')[0], exist_ok=True)

"""# Download immagini"""

for chiave in URLIMMAGINI:
  wget.download(URLIMMAGINI[chiave], f'{DIRVOLTI}/{chiave}.jpg')

"""# Download video"""

ydl_opts = {
    'format': 'best',
    'outtmpl': PATHVIDEO,
    'nocheckcertificate': True,
    'skip_download': False,
}


with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download([URLVIDEO])

"""# Ritaglio scena"""

input_file = PATHVIDEO

output_file = PATHVIDEOTAGLIATO

ffmpeg.input(input_file, ss=SECONDOINIZIO, t=DURATAVIDEO).output(output_file).run()

"""# YOLO"""

model = YOLO("yolo11n.pt")
results = model.track(source=PATHVIDEOTAGLIATO, conf=0.3, iou=0.5, stream=True)

"""# Classe persona"""

class Persona:
    def __init__(self, nome, path_immagine):
        self.nome = nome
        self.pathImmagine = path_immagine
        self.id = None
        self.list_ID = []
        self.confidence_scores = []
        self.identification_count = 0

    def update(self, new_ID, confidence=None):
        self.list_ID.append(new_ID)
        self.id = new_ID
        self.identification_count += 1

        if confidence is not None:
            self.confidence_scores.append(confidence)

    def get_statistics(self):
        avg_confidence = sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0
        return {
            'name': self.nome,
            'current_id': self.id,
            'total_ids': len(self.list_ID),
            'identification_count': self.identification_count,
            'average_confidence': avg_confidence,
            'all_ids': self.list_ID.copy()
        }

    def __str__(self) -> str:
        id_history = f"[{', '.join(map(str, self.list_ID[-3:]))}]" if len(self.list_ID) > 0 else "[]"
        avg_conf = sum(self.confidence_scores) / len(self.confidence_scores) if self.confidence_scores else 0
        return f'{self.nome} | Current ID: {self.id} | IDs: {id_history} | Identifications: {self.identification_count} | Avg Conf: {avg_conf:.3f}'

    def __repr__(self) -> str:
        return self.__str__()

"""# Creazione dizionario"""

dizionario = {}
image_extensions = ('.jpg', '.jpeg', '.png', '.bmp')
face_files = [name for name in os.listdir(DIRVOLTI) if name.lower().endswith(image_extensions)]

for name in face_files:
    person_name = name.split('.')[0]
    full_path = f'{DIRVOLTI}/{name}'

    if os.path.exists(full_path):
        dizionario[person_name] = Persona(person_name, full_path)
    else:
        print(f"Error: File not found: {full_path}")

"""# Rimozioni file"""

def rimuovi_file_in_directory(directory):
    """Rimuove tutti i file in una directory"""
    if os.path.exists(directory):
        files = glob.glob(os.path.join(directory, '*'))
        for f in files:
            try:
                if os.path.isfile(f) or os.path.islink(f):
                    os.unlink(f)
                elif os.path.isdir(f):
                    shutil.rmtree(f)
            except Exception as e:
                pass

cache_files = ['cache_embeddings_volti.pkl', 'cache_embeddings_volti_auraface.pkl', 'cache_embeddings_volti_buffalo_l.pkl']
for cache_file in cache_files:
    if os.path.exists(cache_file):
        os.remove(cache_file)

auraface_dir = "models/auraface"

rimuovi_file_in_directory(auraface_dir)

"""# Classe RiconoscitoreFacciale"""

class RiconoscitoreFacciale:
    """
    Sistema di riconoscimento facciale con supporto per modelli ONNX e AuraFace come fallback.
    Rimossa dipendenza da InsightFace.
    """

    def __init__(self, nome_modello=None, percorso_modello=None):
        """
        Inizializza il riconoscitore facciale.

        Args:
            nome_modello: Nome specifico del modello da usare
            percorso_modello: Percorso personalizzato del modello
        """
        self.nome_modello_richiesto = nome_modello
        self.percorso_modello = percorso_modello

        # Stato interno
        self.modello_attivo = "unknown"
        self.session = None
        self.input_name = None
        self.output_names = None
        self.embeddings_noti = []
        self.nomi_noti = []

        # Inizializza il modello migliore disponibile
        self._inizializza_modello()

        # File cache specifico per modello
        self.file_cache = f'cache_embeddings_{self.modello_attivo.lower()}.pkl'

    def _inizializza_modello(self):
        """Prova a inizializzare i modelli in ordine di priorità."""
        # Lista modelli da provare in ordine
        modelli = []

        if self.nome_modello_richiesto:
            modelli.append(('custom', self.nome_modello_richiesto))

        if self.percorso_modello:
            modelli.append(('onnx_custom', self.percorso_modello))

        # AuraFace come fallback
        modelli.append(('auraface', None))

        # Prova ogni modello fino a trovarne uno funzionante
        for tipo, nome in modelli:
            if self._carica_modello(tipo, nome):
                print(f"Modello '{self.modello_attivo}' caricato con successo")
                return

        raise Exception("Nessun modello di riconoscimento facciale disponibile")

    def _carica_modello(self, tipo, nome=None):
        """
        Carica un tipo specifico di modello.

        Returns:
            bool: True se il caricamento è riuscito
        """
        try:
            if tipo == 'custom' or tipo == 'onnx_custom':
                percorso = nome if tipo == 'custom' else self.percorso_modello
                if percorso and str(percorso).endswith('.onnx'):
                    return self._carica_modello_onnx_diretto(percorso)
            elif tipo == 'auraface':
                return self._carica_auraface_onnx()

        except Exception as e:
            print(f"Errore caricamento {tipo}: {e}")
            return False

    def _carica_auraface_onnx(self):
        """Carica AuraFace come modello ONNX diretto."""
        from huggingface_hub import snapshot_download

        # Scarica modello se necessario
        auraface_dir = "models/auraface"
        os.makedirs(auraface_dir, exist_ok=True)

        try:
            snapshot_download("fal/AuraFace-v1", local_dir=auraface_dir)

            # Cerca file ONNX nella cartella
            onnx_files = [f for f in os.listdir(auraface_dir) if f.endswith('.onnx')]

            if not onnx_files:
                print("Nessun file ONNX trovato in AuraFace")
                return False

            # Usa il primo file ONNX trovato
            percorso_onnx = os.path.join(auraface_dir, onnx_files[0])

            return self._carica_modello_onnx_diretto(percorso_onnx, nome_modello="AuraFace")

        except Exception as e:
            print(f"Errore download/caricamento AuraFace: {e}")
            return False

    def _carica_modello_onnx_diretto(self, percorso_onnx, nome_modello=None):
        """Carica direttamente un file .onnx."""
        import onnxruntime as ort

        if not os.path.exists(percorso_onnx):
            print(f"File ONNX non trovato: {percorso_onnx}")
            return False

        # Configura i provider
        providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]

        # Carica il modello
        self.session = ort.InferenceSession(percorso_onnx, providers=providers)

        # Ottieni informazioni su input/output
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]

        # Imposta il nome del modello
        if nome_modello:
            self.modello_attivo = nome_modello
        else:
            nome_file = os.path.basename(percorso_onnx)
            self.modello_attivo = os.path.splitext(nome_file)[0]

        print(f"Modello ONNX caricato: {self.modello_attivo}")
        print(f"Input: {self.input_name}")
        print(f"Output: {self.output_names}")

        return True

    def _preprocessa_immagine_onnx(self, immagine):
        """Preprocessa l'immagine per modelli ONNX diretti."""
        # Ridimensiona a 112x112 (standard per ArcFace)
        img_resized = cv2.resize(immagine, (112, 112))

        # Normalizza
        img_normalized = (img_resized.astype(np.float32) - 127.5) / 127.5

        # Converti da HWC a CHW e aggiungi batch dimension
        img_transposed = np.transpose(img_normalized, (2, 0, 1))
        img_batch = np.expand_dims(img_transposed, axis=0)

        return img_batch

    def _rileva_volti_semplice(self, immagine):
        """Rilevamento volti semplice usando OpenCV per modelli ONNX diretti."""
        # Carica il classificatore Haar per volti
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

        # Converti in scala di grigi
        gray = cv2.cvtColor(immagine, cv2.COLOR_BGR2GRAY)

        # Rileva volti
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)

        return faces

    def estrai_embedding(self, percorso_immagine):
        """
        Estrae l'embedding facciale da un'immagine usando modelli ONNX.

        Returns:
            numpy.array: Embedding del volto o None se nessun volto trovato
        """
        try:
            # Carica immagine
            immagine = cv2.imread(percorso_immagine)
            if immagine is None:
                print(f"Impossibile caricare l'immagine: {percorso_immagine}")
                return None

            return self._estrai_embedding_onnx(immagine)

        except Exception as e:
            print(f"Errore estrazione embedding: {e}")
            return None

    def _estrai_embedding_onnx(self, immagine):
        """Estrae embedding usando modello ONNX diretto."""
        # Rileva volti con OpenCV
        faces = self._rileva_volti_semplice(immagine)

        if len(faces) == 0:
            return None

        # Prendi il volto più grande
        face = max(faces, key=lambda x: x[2] * x[3])
        x, y, w, h = face

        # Estrai ROI del volto
        face_roi = immagine[y:y+h, x:x+w]

        # Preprocessa per il modello
        input_data = self._preprocessa_immagine_onnx(face_roi)

        # Esegui inferenza
        outputs = self.session.run(self.output_names, {self.input_name: input_data})

        # Restituisci il primo output (embedding)
        embedding = outputs[0][0]

        # Normalizza l'embedding
        embedding = embedding / np.linalg.norm(embedding)

        return embedding

    def carica_volti_noti(self, cartella_volti):
        """Carica gli embeddings dei volti noti dalla cartella o dalla cache."""
        # Prova a caricare dalla cache
        if self._carica_cache():
            return

        # Elabora le immagini della cartella
        if not os.path.exists(cartella_volti):
            print(f"Cartella non trovata: {cartella_volti}")
            return

        # Trova tutti i file immagine
        estensioni_valide = ('.png', '.jpg', '.jpeg', '.bmp')
        file_immagini = [f for f in os.listdir(cartella_volti) if f.lower().endswith(estensioni_valide)]

        print(f"Elaborazione {len(file_immagini)} immagini...")

        # Processa ogni immagine
        for file_img in file_immagini:
            percorso_completo = os.path.join(cartella_volti, file_img)
            embedding = self.estrai_embedding(percorso_completo)
            if embedding is not None:
                start_time = time.time()
                nome_persona = os.path.splitext(file_img)[0]
                self.embeddings_noti.append(embedding)
                self.nomi_noti.append(nome_persona)
                end_time = time.time()
                stats.aggiungi_tempistiche_embeddings(nome_persona, (end_time - start_time))
                print(f"{nome_persona}")

        # Salva cache se ci sono embeddings
        if self.embeddings_noti:
            self._salva_cache()
            print(f"Cache salvata: {len(self.embeddings_noti)} volti")

    def _carica_cache(self):
        """Carica embeddings dalla cache se compatibile."""
        if not os.path.exists(self.file_cache):
            return False

        try:
            with open(self.file_cache, 'rb') as f:
                dati = pickle.load(f)

            # Verifica compatibilità modello
            if dati.get('modello') == self.modello_attivo:
                self.embeddings_noti = dati['embeddings']
                self.nomi_noti = dati['nomi']
                print(f"Cache caricata ({len(self.embeddings_noti)} volti)")
                return True

        except Exception as e:
            print(f"Errore caricamento cache: {e}")

        return False

    def _salva_cache(self):
        """Salva embeddings nella cache."""
        try:
            dati_cache = {
                'embeddings': self.embeddings_noti,
                'nomi': self.nomi_noti,
                'modello': self.modello_attivo
            }
            with open(self.file_cache, 'wb') as f:
                pickle.dump(dati_cache, f)
        except Exception as e:
            print(f"Errore salvataggio cache: {e}")

    def identifica_volto(self, percorso_immagine, soglia=0.4):
        """
        Identifica un volto confrontandolo con il database.

        Returns:
            tuple: (nome_persona, confidenza) o ('-1', confidenza) se non riconosciuto
        """
        if not self.embeddings_noti:
            return '-1', 0.0

        # Estrai embedding dall'immagine
        embedding = self.estrai_embedding(percorso_immagine)
        if embedding is None:
            return '-1', 0.0

        # Calcola similarità con tutti i volti noti
        embedding = embedding.reshape(1, -1)
        embeddings_db = np.array(self.embeddings_noti)
        similarita = cosine_similarity(embedding, embeddings_db)[0]

        # Trova il match migliore
        indice_migliore = np.argmax(similarita)
        confidenza_migliore = similarita[indice_migliore]

        # Verifica se supera la soglia
        if confidenza_migliore >= soglia:
            return self.nomi_noti[indice_migliore], confidenza_migliore

        return '-1', confidenza_migliore

    def aggiungi_volto(self, percorso_immagine, nome_persona):
        """Aggiunge un nuovo volto al database."""

        start_time = time.time()
        embedding = self.estrai_embedding(percorso_immagine)
        end_time = time.time()
        stats.aggiungi_tempistiche_embeddings(nome_persona, (end_time - start_time))

        if embedding is None:
            print(f"Impossibile estrarre volto da {percorso_immagine}")
            return False

        # Aggiungi al database
        self.embeddings_noti.append(embedding)
        self.nomi_noti.append(nome_persona)

        # Aggiorna cache
        self._salva_cache()
        print(f"{nome_persona} aggiunto al database")
        return True

    def get_info_modello(self):
        """Restituisce informazioni sul modello attivo."""
        return {
            'modello_attivo': self.modello_attivo,
            'tipo_modello': 'onnx_diretto',
            'modello_richiesto': self.nome_modello_richiesto,
            'volti_nel_database': len(self.embeddings_noti)
        }

"""# Classe StatisticheRiconoscimento"""

class StatisticheRiconoscimento:
    def __init__(self):
        self.identificazioni_riuscite = 0
        self.tentativi_falliti = 0
        self.punteggi_confidenza = []
        self.frame_processati = 0
        self.tempistiche_embeddings = {}
        self.avg_tempistiche_embeddings = -1
        self.tempo_elaborazione_video = None
        self.tempo_matching_volti_best = {}
        self.tempo_matching_volti_all = {}

    def aggiungi_tempistiche_embeddings(self, nome, durata):
        """Aggiunge una tempistica per la generazione degli embeddings."""
        self.tempistiche_embeddings.update({nome: durata})

    def set_elaborazione_video(self, durata):
        """Imposta il tempo totale di elaborazione del video."""
        self.tempo_elaborazione_video = durata

    def aggiungi_tempistiche_matching_volti_all(self, nome, durata):
        """Aggiunge una tempistica di matching per un volto specifico."""
        if nome in self.tempo_matching_volti_all:
            self.tempo_matching_volti_all[nome].append(durata)
        else:
            self.tempo_matching_volti_all[nome] = [durata]

    def calcola_avg_tempistiche_embeddings(self):
        """Calcola il tempo medio per la generazione degli embeddings."""
        if len(self.tempistiche_embeddings) > 0:
            self.avg_tempistiche_embeddings = sum(self.tempistiche_embeddings.values()) / len(self.tempistiche_embeddings)
        else:
            self.avg_tempistiche_embeddings = 0

    def calcola_volti_best(self):
        """Calcola i tempi migliori di matching per ogni volto."""
        for key in self.tempo_matching_volti_all:
            self.tempo_matching_volti_best.update({key: min(self.tempo_matching_volti_all[key])})

    def aggiungi_successo(self, confidenza):
        """Registra un'identificazione riuscita con il suo punteggio di confidenza."""
        self.identificazioni_riuscite += 1
        self.punteggi_confidenza.append(confidenza)

    def aggiungi_fallimento(self):
        """Registra un tentativo di identificazione fallito."""
        self.tentativi_falliti += 1

    def incrementa_frame(self):
        """Incrementa il contatore dei frame processati."""
        self.frame_processati += 1

    def calcola_statistiche(self):
        """
        Calcola e restituisce tutte le statistiche finali del sistema.

        Returns:
            dict: Dizionario contenente tutte le metriche di performance del sistema
        """
        self.calcola_volti_best()
        self.calcola_avg_tempistiche_embeddings()

        tentativi_totali = self.identificazioni_riuscite + self.tentativi_falliti
        tasso_successo = (self.identificazioni_riuscite / tentativi_totali * 100) if tentativi_totali > 0 else 0
        confidenza_media = sum(self.punteggi_confidenza) / len(self.punteggi_confidenza) if self.punteggi_confidenza else 0

        return {
            'frame_processati': self.frame_processati,
            'identificazioni_riuscite': self.identificazioni_riuscite,
            'tentativi_falliti': self.tentativi_falliti,
            'tasso_successo': tasso_successo,
            'confidenza_media': confidenza_media,
            'tempistiche_embeddings_media': self.avg_tempistiche_embeddings,
            'tempistiche_embeddings':self.tempistiche_embeddings,
            'tempo_elaborazione_video': self.tempo_elaborazione_video,
            'tempo_matching_volti_best': self.tempo_matching_volti_best,
            'tempo_matching_volti_all': self.tempo_matching_volti_all
        }

"""# Dichiarazione funzioni"""

def identifica_persona(percorso_immagine, soglia_confidenza=0.55):
    """
    Identifica una persona dall'immagine ritagliata con tracciamento delle tempistiche.
    """
    if not os.path.exists(percorso_immagine):
        return '-1', 0.0

    try:
        start_time = time.time()

        nome, confidenza = riconoscitore_volti.identifica_volto(percorso_immagine, soglia_confidenza)

        matching_time = time.time() - start_time
        if nome != '-1':
            stats.aggiungi_tempistiche_matching_volti_all(nome, matching_time)

        return nome, confidenza
    except Exception:
        return '-1', 0.0

def e_persona_conosciuta(id_tracciamento):
    """Verifica se la persona è già stata identificata."""
    return id_tracciamento is not None and id_tracciamento in id_conosciuti

def e_ritaglio_valido(immagine, dimensione_minima=40):
    """
    Verifica se l'immagine è adatta per il riconoscimento.

    Args:
        immagine: Array numpy dell'immagine
        dimensione_minima: Dimensione minima richiesta

    Returns:
        bool: True se l'immagine è valida
    """
    if immagine is None or immagine.size == 0:
        return False

    altezza, larghezza = immagine.shape[:2]
    return min(altezza, larghezza) >= dimensione_minima

def estrai_coordinate_box(box, larghezza_frame, altezza_frame, margine=20):
    """
    Estrae le coordinate del bounding box con margine di sicurezza.

    Returns:
        tuple: (x1, y1, x2, y2) coordinate corrette
    """
    coordinate_xyxy = box.xyxy[0].cpu().numpy().astype(int)

    x1 = max(0, coordinate_xyxy[0] - margine)
    y1 = max(0, coordinate_xyxy[1] - margine)
    x2 = min(larghezza_frame, coordinate_xyxy[2] + margine)
    y2 = min(altezza_frame, coordinate_xyxy[3] + margine)

    return x1, y1, x2, y2

def aggiorna_tracciamento_persona(nome_identificato, id_tracciamento, confidenza):
    """
    Aggiorna il tracciamento di una persona identificata.

    Args:
        nome_identificato: Nome della persona identificata
        id_tracciamento: ID di tracciamento corrente
        confidenza: Punteggio di confidenza
    """
    if nome_identificato not in dizionario:
        return

    persona = dizionario[nome_identificato]
    vecchio_id = persona.id

    # Aggiorna i dati della persona
    persona.update(id_tracciamento, confidenza)

    # Gestisce il cambio di ID di tracciamento
    if vecchio_id is not None and vecchio_id != id_tracciamento:
        if vecchio_id in id_conosciuti:
            id_conosciuti.remove(vecchio_id)

    # Aggiunge il nuovo ID al tracciamento
    if id_tracciamento is not None:
        id_conosciuti.add(id_tracciamento)

def processa_rilevazioni(results):
    """
    Processa i risultati delle rilevazioni YOLO per identificare le persone.

    Args:
        results: Risultati delle rilevazioni YOLO
    """
    for indice, risultato in enumerate(results):
        # Salta frame in base al tasso di campionamento
        if indice % FRAMEDASALTARE != 0:
            continue

        stats.incrementa_frame()

        # Verifica se ci sono rilevazioni nel frame
        if risultato.boxes is None:
            continue

        altezza_frame, larghezza_frame = risultato.orig_img.shape[:2]

        # Processa ogni rilevazione nel frame
        for box in risultato.boxes:
            processa_singola_rilevazione(box, risultato.orig_img, larghezza_frame, altezza_frame, stats)

def processa_singola_rilevazione(box, immagine_frame, larghezza_frame, altezza_frame, stats):
    """
    Processa una singola rilevazione di persona.

    Args:
        box: Bounding box della rilevazione
        immagine_frame: Immagine del frame corrente
        larghezza_frame, altezza_frame: Dimensioni del frame
        stats: Oggetto statistiche per il tracciamento
    """
    id_classe = int(box.cls)
    id_tracciamento = box.id.item() if box.id is not None else None

    # Processa solo rilevazioni di persone (classe 0)
    if id_classe != 0:
        return

    # Salta persone già identificate
    if e_persona_conosciuta(id_tracciamento):
        return

    # Estrae e valida l'immagine ritagliata
    x1, y1, x2, y2 = estrai_coordinate_box(box, larghezza_frame, altezza_frame)
    immagine_ritagliata = immagine_frame[y1:y2, x1:x2]

    if not e_ritaglio_valido(immagine_ritagliata):
        return

    # Tenta l'identificazione della persona
    tentativo_identificazione(immagine_ritagliata, id_tracciamento, stats)

def tentativo_identificazione(immagine_ritagliata, id_tracciamento, stats):
    """
    Esegue il tentativo di identificazione di una persona.

    Args:
        immagine_ritagliata: Immagine della persona ritagliata
        id_tracciamento: ID di tracciamento della persona
        stats: Oggetto statistiche
    """
    file_salvato = False

    try:
        # Salva temporaneamente l'immagine
        cv2.imwrite(TEMPIMG, immagine_ritagliata)
        file_salvato = True

        # Esegue l'identificazione
        nome_identificato, confidenza = identifica_persona(TEMPIMG)

        if nome_identificato != '-1':
            # Identificazione riuscita
            stats.aggiungi_successo(confidenza)
            aggiorna_tracciamento_persona(nome_identificato, id_tracciamento, confidenza)
        else:
            # Identificazione fallita
            stats.aggiungi_fallimento()

    except Exception:
        stats.aggiungi_fallimento()

    finally:
        # Pulizia del file temporaneo
        if file_salvato and os.path.exists(TEMPIMG):
            os.remove(TEMPIMG)

"""# Stampa statistiche"""

stats = StatisticheRiconoscimento()

riconoscitore_volti = RiconoscitoreFacciale()
riconoscitore_volti.carica_volti_noti(DIRVOLTI)

# Estrae gli ID già conosciuti dal dizionario esistente
id_conosciuti = {
    dizionario[chiave].id
    for chiave in dizionario
    if hasattr(dizionario[chiave], 'id') and dizionario[chiave].id is not None
}


start_time_video = time.time()
processa_rilevazioni(results)
start_time_video = time.time()

tempo_analisi_video = start_time_video - start_time_video

stats.set_elaborazione_video(tempo_analisi_video)

statistiche_finali = stats.calcola_statistiche()

# Stampa le statistiche
print(f"Frame processati: {statistiche_finali['frame_processati']}")
print(f"Identificazioni riuscite: {statistiche_finali['identificazioni_riuscite']}")
print(f"Tasso di successo: {statistiche_finali['tasso_successo']:.1f}%")
print(f"Confidenza media: {statistiche_finali['confidenza_media']:.2f}")

"""# Stampa dizionario"""

print(dizionario)

"""https://github.com/onnx/models/tree/main"""

print(statistiche_finali)

for key in statistiche_finali:
    print(f'{key}:{statistiche_finali[key]}')